
1. Selecting the right model
Now that you have learned the basic logics and algorithms of supervised and unsupervised learning, let's go a bit into the specifics of selecting the right model for our application.

2. Model-to-problem fit
Of course, it is first necessary to narrow the competition down to models that are common-sense candidates for our problem. So, do we have a certain target variable, whose values we want to learn to predict, OR are we just discovering patterns in the data? If the target variable exists, are its values categories or numbers? If it doesn't, what is our objective then? Simplify the feature space? Find groups of similar records? Or discover anomalies or novelty in the data?

3. Defining the priorities
Once we have a pool of fitting candidates, we need to start prioritizing. One of the common dilemmas is the choice between model interpretability and performance. The general rule is that simpler models are easier to interpret, while complex ones dominate the performance arena, but there can be exceptions at both extremes. A simple Decision Tree might give you both the required performance AND interpretability, but it can also happen that you train a monster of a Deep Neural network, and achieve neither of the two. It really depends on your problem and your data, but a good mantra is "Simplicity first!". Try to always start with the simpler, faster and interpretable model, and move on up the complexity ladder only if necessary.

4. Using multiple metrics
We already talked about individual metrics, but we rarely select our model based on only one. So how do we select one single best model if different ones excel at different metrics? One good practice is to differentiate between satisfying and optimizing metrics. Satisfying metrics are metrics that define a cut-off criteria that every candidate model needs to meet. Multiple metrics of this kind can be used for the same evaluation, such as minimum accuracy, maximum execution time and so forth. After bad apples have been filtered out, we apply the optimizing metrics. This should be a metric that illustrates the ultimate business priority and there can be only one, like "minimize percentage of undetected diseases" or "minimize the percentage of false alarms" in a fraud detection system. So, the final model is the one that passes the bar on all satisfying metrics and is the highest ranked one on the optimizing metric. In a real life scenario we would probably repeat this "model competition" from time to time, because real-life data changes in time, and therefore so does the ranking of the models.

5. Interpretation
Lastly, once our model is built and ready, we will often want to pop-open the hood and take a peek at what's happening underneath it. There are two most common ways of interpreting the model: global and local interpretation. With global interpretation, we try to figure out "What are the general decision-making rules of this model?". Common approaches in this case include visualizing decision trees, or plotting feature importance. With local interpretation, we are investigating: "Why was this specific example classified in this way?" For example, in the EU, if your bank declines to give you a loan based on a Machine Learning model, you can ask it to provide you the explanation on what the key criteria were when making such a decision. One of the most popular algorithms for local model interpretation is the so-called LIME algorithm.

6. Model selection and interpretation
Model selection and interpretation are the head and tail of a Machine Learning project. Usually, these are the stages where most business problem knowledge is necessary to make the right decisions, so never take them lightly. Let's recap this topic with a couple of exercises.